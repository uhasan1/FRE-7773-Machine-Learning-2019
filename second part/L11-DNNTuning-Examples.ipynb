{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Tuning Examples\n",
    "**Make sure you have activated the correct python envorinment**\n",
    "\n",
    "+ DNN = Dense Neural Network\n",
    "+ Using the keras Sequential API and MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "np.random.seed(42)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.initializers import RandomNormal, he_normal\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "MNIST is a dataset of 60,000 28x28 grayscale images of handwritten digits, along with a test set of 10,000 images.\n",
    "\n",
    "Load the MNIST data using keras. The first time the data are downloaded and cached.  \n",
    "Subsequent times the data are loaded from the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True values: [4 0 0 8 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the date and split into training/testing sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# plot a few digits and print the true values\n",
    "idigits = np.random.randint(0, len(y_train), 5)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "for i in range(len(idigits)):\n",
    "    ax = fig.add_subplot(1, len(idigits), i + 1)\n",
    "    ax.imshow(x_train[idigits[i]], cmap = plt.cm.binary, interpolation=\"nearest\")\n",
    "    ax.axis(\"off\")\n",
    "print('True values: {}'.format(y_train[idigits]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape, cast, normalize inputs\n",
    "* Reshape the data: 28x28 -> 784  \n",
    "* Cast the values int8 -> float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_train = len(x_train)  # 60000\n",
    "n_test = len(x_test)  # 10000\n",
    "input_dim = 28 * 28\n",
    "\n",
    "x_train = x_train.reshape(n_train, input_dim)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.reshape(n_test, input_dim)\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the target values vector to binary class matrix\n",
    "y_train is a vector of size 60,000.  \n",
    "It gets mapped to a 60,000 x 10 matrix of 0s and 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets before: \n",
      "[5 0 4 1 9]\n",
      "Targets after: \n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Targets before: \\n{}\".format(y_train[:5]))\n",
    "ybm_train = to_categorical(y_train, n_classes)\n",
    "ybm_test = to_categorical(y_test, n_classes)\n",
    "print(\"Targets after: \\n{}\".format(ybm_train[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the common network parameters\n",
    "# Number of hidden layers and units per layer\n",
    "hidden = [100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn1 = Sequential()\n",
    "nn1.add(Dense(hidden[0], input_dim=input_dim, activation='relu'))\n",
    "for i in range(len(hidden) - 1):\n",
    "    nn1.add(Dense(hidden[i + 1], activation='relu'))\n",
    "nn1.add(Dense(n_classes, activation='softmax'))\n",
    "nn1.summary()\n",
    "\n",
    "# the number of parameters is: n_classes x (input_dim + 1)\n",
    "# each node has its own bias (therefore +1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/40\n",
      "54000/54000 [==============================] - 1s 20us/step - loss: 2.2215 - acc: 0.2429 - val_loss: 2.0862 - val_acc: 0.3863\n",
      "Epoch 2/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.9558 - acc: 0.4678 - val_loss: 1.7983 - val_acc: 0.5668\n",
      "Epoch 3/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.6681 - acc: 0.6067 - val_loss: 1.4877 - val_acc: 0.6947\n",
      "Epoch 4/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.3793 - acc: 0.6979 - val_loss: 1.1990 - val_acc: 0.7638\n",
      "Epoch 5/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.1345 - acc: 0.7517 - val_loss: 0.9739 - val_acc: 0.8105\n",
      "Epoch 6/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.9540 - acc: 0.7844 - val_loss: 0.8150 - val_acc: 0.8348\n",
      "Epoch 7/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.8270 - acc: 0.8062 - val_loss: 0.7036 - val_acc: 0.8550\n",
      "Epoch 8/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.7364 - acc: 0.8218 - val_loss: 0.6238 - val_acc: 0.8658\n",
      "Epoch 9/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.6699 - acc: 0.8345 - val_loss: 0.5647 - val_acc: 0.8740\n",
      "Epoch 10/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.6194 - acc: 0.8441 - val_loss: 0.5192 - val_acc: 0.8812\n",
      "Epoch 11/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.5797 - acc: 0.8522 - val_loss: 0.4837 - val_acc: 0.8880\n",
      "Epoch 12/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.5476 - acc: 0.8587 - val_loss: 0.4551 - val_acc: 0.8920\n",
      "Epoch 13/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.5213 - acc: 0.8645 - val_loss: 0.4315 - val_acc: 0.8967\n",
      "Epoch 14/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.4991 - acc: 0.8686 - val_loss: 0.4120 - val_acc: 0.9002\n",
      "Epoch 15/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.4803 - acc: 0.8721 - val_loss: 0.3954 - val_acc: 0.9027\n",
      "Epoch 16/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.4641 - acc: 0.8752 - val_loss: 0.3810 - val_acc: 0.9050\n",
      "Epoch 17/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.4499 - acc: 0.8777 - val_loss: 0.3689 - val_acc: 0.9053\n",
      "Epoch 18/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.4374 - acc: 0.8804 - val_loss: 0.3580 - val_acc: 0.9078\n",
      "Epoch 19/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.4264 - acc: 0.8828 - val_loss: 0.3483 - val_acc: 0.9087\n",
      "Epoch 20/40\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.4165 - acc: 0.8847 - val_loss: 0.3403 - val_acc: 0.9118\n",
      "Epoch 21/40\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.4076 - acc: 0.8867 - val_loss: 0.3324 - val_acc: 0.9127\n",
      "Epoch 22/40\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.3995 - acc: 0.8888 - val_loss: 0.3254 - val_acc: 0.9145\n",
      "Epoch 23/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3921 - acc: 0.8903 - val_loss: 0.3195 - val_acc: 0.9148\n",
      "Epoch 24/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3854 - acc: 0.8924 - val_loss: 0.3137 - val_acc: 0.9163\n",
      "Epoch 25/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3791 - acc: 0.8936 - val_loss: 0.3084 - val_acc: 0.9180\n",
      "Epoch 26/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3732 - acc: 0.8953 - val_loss: 0.3040 - val_acc: 0.9190\n",
      "Epoch 27/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3679 - acc: 0.8966 - val_loss: 0.2993 - val_acc: 0.9203\n",
      "Epoch 28/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3629 - acc: 0.8980 - val_loss: 0.2951 - val_acc: 0.9198\n",
      "Epoch 29/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3582 - acc: 0.8989 - val_loss: 0.2912 - val_acc: 0.9220\n",
      "Epoch 30/40\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.3537 - acc: 0.9001 - val_loss: 0.2875 - val_acc: 0.9240\n",
      "Epoch 31/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3495 - acc: 0.9009 - val_loss: 0.2841 - val_acc: 0.9242\n",
      "Epoch 32/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3454 - acc: 0.9021 - val_loss: 0.2809 - val_acc: 0.9252\n",
      "Epoch 33/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3417 - acc: 0.9030 - val_loss: 0.2778 - val_acc: 0.9258\n",
      "Epoch 34/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3380 - acc: 0.9042 - val_loss: 0.2749 - val_acc: 0.9262\n",
      "Epoch 35/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3345 - acc: 0.9049 - val_loss: 0.2718 - val_acc: 0.9273\n",
      "Epoch 36/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3313 - acc: 0.9060 - val_loss: 0.2694 - val_acc: 0.9275\n",
      "Epoch 37/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3281 - acc: 0.9064 - val_loss: 0.2670 - val_acc: 0.9275\n",
      "Epoch 38/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3250 - acc: 0.9074 - val_loss: 0.2645 - val_acc: 0.9287\n",
      "Epoch 39/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3221 - acc: 0.9079 - val_loss: 0.2623 - val_acc: 0.9282\n",
      "Epoch 40/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3193 - acc: 0.9088 - val_loss: 0.2600 - val_acc: 0.9295\n",
      "Default DNN\n",
      "------------------------\n",
      "Test loss score: 0.2985\n",
      "Test accuracy:   0.9143\n"
     ]
    }
   ],
   "source": [
    "nn1.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 1000\n",
    "n_epochs = 40\n",
    "val_split = 0.1 \n",
    "# create an early stopping callback\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "nn1_hist = nn1.fit(x_train, ybm_train, batch_size=batch_size, epochs=n_epochs, validation_split=val_split, callbacks=[es], verbose=1)\n",
    "val_score = nn1.evaluate(x_test, ybm_test, verbose=0)\n",
    "\n",
    "print('Default DNN')\n",
    "print('------------------------')\n",
    "print('Test loss score: {0:.4f}'.format(val_score[0]))\n",
    "print('Test accuracy:   {0:.4f}'.format(val_score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN with recommended settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Dropout rate\n",
    "drop_rate = 0.4\n",
    "# Create the initializer\n",
    "# ini = RandomNormal()\n",
    "ini = he_normal()\n",
    "\n",
    "nn2 = Sequential()\n",
    "nn2.add(Dense(hidden[0], input_dim=input_dim, activation='elu', kernel_initializer=ini))\n",
    "nn2.add(Dropout(drop_rate))\n",
    "for i in range(len(hidden) - 1):\n",
    "    nn2.add(Dense(hidden[i + 1], activation='elu', kernel_initializer=ini))\n",
    "    nn2.add(Dropout(drop_rate))    \n",
    "nn2.add(Dense(n_classes, activation='softmax', kernel_initializer=ini))\n",
    "nn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/40\n",
      "54000/54000 [==============================] - 1s 26us/step - loss: 0.8116 - acc: 0.7426 - val_loss: 0.2613 - val_acc: 0.9297\n",
      "Epoch 2/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.4143 - acc: 0.8760 - val_loss: 0.2181 - val_acc: 0.9373\n",
      "Epoch 3/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.3620 - acc: 0.8930 - val_loss: 0.1962 - val_acc: 0.9435\n",
      "Epoch 4/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.3254 - acc: 0.9029 - val_loss: 0.1726 - val_acc: 0.9488\n",
      "Epoch 5/40\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.3009 - acc: 0.9111 - val_loss: 0.1612 - val_acc: 0.9548\n",
      "Epoch 6/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2801 - acc: 0.9164 - val_loss: 0.1472 - val_acc: 0.9587\n",
      "Epoch 7/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2625 - acc: 0.9217 - val_loss: 0.1354 - val_acc: 0.9613\n",
      "Epoch 8/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2443 - acc: 0.9255 - val_loss: 0.1309 - val_acc: 0.9633\n",
      "Epoch 9/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2367 - acc: 0.9278 - val_loss: 0.1196 - val_acc: 0.9645\n",
      "Epoch 10/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2260 - acc: 0.9314 - val_loss: 0.1172 - val_acc: 0.9667\n",
      "Epoch 11/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2147 - acc: 0.9341 - val_loss: 0.1125 - val_acc: 0.9685\n",
      "Epoch 12/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2091 - acc: 0.9364 - val_loss: 0.1077 - val_acc: 0.9693\n",
      "Epoch 13/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.1993 - acc: 0.9389 - val_loss: 0.1021 - val_acc: 0.9707\n",
      "Epoch 14/40\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.1925 - acc: 0.9405 - val_loss: 0.1003 - val_acc: 0.9708\n",
      "Epoch 15/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.1858 - acc: 0.9434 - val_loss: 0.1001 - val_acc: 0.9705\n",
      "Epoch 16/40\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.1811 - acc: 0.9443 - val_loss: 0.0934 - val_acc: 0.9733\n",
      "Epoch 17/40\n",
      "54000/54000 [==============================] - 2s 32us/step - loss: 0.1787 - acc: 0.9447 - val_loss: 0.0928 - val_acc: 0.9733\n",
      "Epoch 18/40\n",
      "54000/54000 [==============================] - 2s 31us/step - loss: 0.1723 - acc: 0.9469 - val_loss: 0.0899 - val_acc: 0.9732\n",
      "Epoch 19/40\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.1647 - acc: 0.9506 - val_loss: 0.0877 - val_acc: 0.9740\n",
      "Epoch 20/40\n",
      "54000/54000 [==============================] - 1s 22us/step - loss: 0.1613 - acc: 0.9495 - val_loss: 0.0893 - val_acc: 0.9737\n",
      "Epoch 21/40\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.1580 - acc: 0.9512 - val_loss: 0.0880 - val_acc: 0.9742\n",
      "Epoch 22/40\n",
      "54000/54000 [==============================] - 2s 31us/step - loss: 0.1549 - acc: 0.9523 - val_loss: 0.0846 - val_acc: 0.9738\n",
      "Epoch 23/40\n",
      "54000/54000 [==============================] - 2s 30us/step - loss: 0.1516 - acc: 0.9524 - val_loss: 0.0856 - val_acc: 0.9757\n",
      "Epoch 24/40\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.1484 - acc: 0.9534 - val_loss: 0.0838 - val_acc: 0.9752\n",
      "Epoch 25/40\n",
      "54000/54000 [==============================] - 1s 22us/step - loss: 0.1446 - acc: 0.9551 - val_loss: 0.0811 - val_acc: 0.9757\n",
      "Epoch 26/40\n",
      "54000/54000 [==============================] - 1s 21us/step - loss: 0.1435 - acc: 0.9558 - val_loss: 0.0802 - val_acc: 0.9763\n",
      "Epoch 27/40\n",
      "54000/54000 [==============================] - 1s 22us/step - loss: 0.1376 - acc: 0.9571 - val_loss: 0.0800 - val_acc: 0.9757\n",
      "Epoch 28/40\n",
      "54000/54000 [==============================] - 1s 22us/step - loss: 0.1367 - acc: 0.9573 - val_loss: 0.0770 - val_acc: 0.9767\n",
      "Epoch 29/40\n",
      "54000/54000 [==============================] - 1s 23us/step - loss: 0.1332 - acc: 0.9588 - val_loss: 0.0772 - val_acc: 0.9775\n",
      "Epoch 30/40\n",
      "54000/54000 [==============================] - 1s 21us/step - loss: 0.1334 - acc: 0.9580 - val_loss: 0.0775 - val_acc: 0.9788\n",
      "Epoch 31/40\n",
      "54000/54000 [==============================] - 1s 21us/step - loss: 0.1317 - acc: 0.9592 - val_loss: 0.0774 - val_acc: 0.9765\n",
      "3-Layer DNN\n",
      "------------------------\n",
      "Test loss score: 0.0868\n",
      "Test accuracy:   0.9738\n"
     ]
    }
   ],
   "source": [
    "# compile, fit and evaluate\n",
    "nn2.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size2 = 1000\n",
    "n_epochs2 = 40\n",
    "val_split2 = 0.1 \n",
    "# create an early stopping callback\n",
    "es2 = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "nn2_hist = nn2.fit(x_train, ybm_train, batch_size=batch_size2, epochs=n_epochs2, validation_split=val_split2, callbacks=[es2], verbose=1)\n",
    "val_score = nn2.evaluate(x_test, ybm_test, verbose=0)\n",
    "\n",
    "print('3-Layer DNN')\n",
    "print('------------------------')\n",
    "print('Test loss score: {0:.4f}'.format(val_score[0]))\n",
    "print('Test accuracy:   {0:.4f}'.format(val_score[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN predictions and true values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn2_pred = nn2.predict(x_test[:20])\n",
    "nn2_ypred = np.argmax(nn2_pred, axis=1)\n",
    "\n",
    "print('DNN predictions and true values')\n",
    "display(nn2_ypred.tolist())\n",
    "display(y_test[:20].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "\n",
    "We fit a multiclass logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1e-05, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=1000, multi_class='multinomial',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "lgm = LogisticRegression(C=1e-5, multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "print(lgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1e-05, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgm.fit(x_train,  y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "------------------------\n",
      "Test accuracy:   0.7482\n"
     ]
    }
   ],
   "source": [
    "y_pred = lgm.predict(x_test)\n",
    "lg_acc = accuracy_score(y_test, y_pred)\n",
    "print('Logistic Regression')\n",
    "print('------------------------')\n",
    "print('Test accuracy:   {0:.4f}'.format(lg_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
